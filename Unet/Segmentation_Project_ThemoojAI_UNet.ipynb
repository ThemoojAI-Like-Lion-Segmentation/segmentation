{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge natsort ipykernel matplotlib pandas numpy opencv tqdm scikit-learn torch torchvision pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import natsort\n",
    "\n",
    "base_path = \"/home/work/The-moojAI/MVTec_data/\"\n",
    "\n",
    "\n",
    "# toothbrush\n",
    "toothbrush_good_img_path = natsort.natsorted(glob.glob(base_path + \"toothbrush/train/good/*\"))\n",
    "toothbrush_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"toothbrush/test/good/*\")))\n",
    "\n",
    "toothbrush_path = natsort.natsorted(glob.glob(base_path + \"toothbrush/test/defective/*\"))\n",
    "toothbrush_gt_path = natsort.natsorted(glob.glob(base_path + \"toothbrush/ground_truth/defective/*\")) # 1\n",
    "\n",
    "\n",
    "# tile\n",
    "tile_good_img_path = natsort.natsorted(glob.glob(base_path + \"tile/train/good/*\"))\n",
    "tile_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"tile/test/good/*\")))\n",
    "\n",
    "tile_rough_path = natsort.natsorted(glob.glob(base_path + \"tile/test/rough/*\"))\n",
    "tile_rough_gt_path = natsort.natsorted(glob.glob(base_path + \"tile/ground_truth/rough/*\")) # 2\n",
    "\n",
    "tile_oil_path = natsort.natsorted(glob.glob(base_path + \"tile/test/oil/*\"))\n",
    "tile_oil_gt_path = natsort.natsorted(glob.glob(base_path + \"tile/ground_truth/oil/*\")) # 3\n",
    "\n",
    "tile_gray_stroke_path = natsort.natsorted(glob.glob(base_path + \"tile/test/gray_stroke/*\"))\n",
    "tile_gray_stroke_gt_path = natsort.natsorted(glob.glob(base_path + \"tile/ground_truth/gray_stroke/*\")) # 4\n",
    "\n",
    "tile_glue_strip_path = natsort.natsorted(glob.glob(base_path + \"tile/test/glue_strip/*\"))\n",
    "tile_glue_strip_gt_path = natsort.natsorted(glob.glob(base_path + \"tile/ground_truth/glue_strip/*\")) # 5\n",
    "\n",
    "tile_crack_path = natsort.natsorted(glob.glob(base_path + \"tile/test/crack/*\"))\n",
    "tile_crack_gt_path = natsort.natsorted(glob.glob(base_path + \"tile/ground_truth/crack/*\")) # 6\n",
    "\n",
    "# screw\n",
    "screw_good_img_path = natsort.natsorted(glob.glob(base_path + \"screw/train/good/*\"))\n",
    "screw_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"screw/test/good/*\")))\n",
    "\n",
    "\n",
    "screw_thread_path = natsort.natsorted(glob.glob(base_path + \"screw/test/thread_top/*\"))\n",
    "screw_thread_path.extend(natsort.natsorted(glob.glob(base_path + \"screw/test/thread_side/*\")))\n",
    "\n",
    "screw_thread_gt_path = natsort.natsorted(glob.glob(base_path + \"screw/ground_truth/thread_top/*\"))\n",
    "screw_thread_gt_path.extend(natsort.natsorted(glob.glob(base_path + \"screw/ground_truth/thread_side/*\"))) # 7\n",
    "\n",
    "screw_scratch_path = natsort.natsorted(glob.glob(base_path + \"screw/test/scratch_neck/*\"))\n",
    "screw_scratch_path.extend(natsort.natsorted(glob.glob(base_path + \"screw/test/scratch_head/*\")))\n",
    "\n",
    "screw_scratch_gt_path = natsort.natsorted(glob.glob(base_path + \"screw/ground_truth/scratch_neck/*\"))\n",
    "screw_scratch_gt_path.extend(natsort.natsorted(glob.glob(base_path + \"screw/ground_truth/scratch_head/*\"))) # 8\n",
    "\n",
    "screw_manipulated_path = natsort.natsorted(glob.glob(base_path + \"screw/test/manipulated_front/*\"))\n",
    "\n",
    "screw_manipulated_gt_path = natsort.natsorted(glob.glob(base_path + \"screw/ground_truth/manipulated_front/*\")) # 9\n",
    "\n",
    "\n",
    "# pill\n",
    "pill_good_img_path = natsort.natsorted(glob.glob(base_path + \"pill/train/good/*\"))\n",
    "pill_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"pill/test/good/*\")))\n",
    "\n",
    "pill_scratch_path = natsort.natsorted(glob.glob(base_path + \"pill/test/scratch/*\"))\n",
    "pill_scratch_gt_path = natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/scratch/*\")) # 10\n",
    "\n",
    "pill_pill_type_path = natsort.natsorted(glob.glob(base_path + \"pill/test/pill_type/*\"))\n",
    "pill_pill_type_gt_path = natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/pill_type/*\")) # 11\n",
    "\n",
    "pill_faulty_imprint_path = natsort.natsorted(glob.glob(base_path + \"pill/test/faulty_imprint/*\"))\n",
    "pill_faulty_imprint_gt_path = natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/faulty_imprint/*\")) # 12\n",
    "\n",
    "pill_crack_path = natsort.natsorted(glob.glob(base_path + \"pill/test/crack/*\"))\n",
    "pill_crack_path.extend(natsort.natsorted(glob.glob(base_path + \"pill/test/contamination/*\")))\n",
    "\n",
    "pill_crack_gt_path = natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/crack/*\"))\n",
    "pill_crack_gt_path.extend(natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/contamination/*\"))) # 13\n",
    "\n",
    "pill_color_path = natsort.natsorted(glob.glob(base_path + \"pill/test/color/*\"))\n",
    "pill_color_gt_path = natsort.natsorted(glob.glob(base_path + \"pill/ground_truth/color/*\")) # 14\n",
    "\n",
    "\n",
    "# capsule\n",
    "capsule_good_img_path = natsort.natsorted(glob.glob(base_path + \"capsule/train/good/*\"))\n",
    "capsule_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"capsule/test/good/*\")))\n",
    "\n",
    "capsule_squeeze_path = natsort.natsorted(glob.glob(base_path + \"capsule/test/squeeze/*\"))\n",
    "capsule_squeeze_gt_path = natsort.natsorted(glob.glob(base_path + \"capsule/ground_truth/squeeze/*\")) # 15\n",
    "\n",
    "capsule_scratch_path = natsort.natsorted(glob.glob(base_path + \"capsule/test/scratch/*\"))\n",
    "capsule_scratch_gt_path = natsort.natsorted(glob.glob(base_path + \"capsule/ground_truth/scratch/*\")) # 16\n",
    "\n",
    "capsule_poke_path = natsort.natsorted(glob.glob(base_path + \"capsule/test/poke/*\"))\n",
    "capsule_poke_gt_path = natsort.natsorted(glob.glob(base_path + \"capsule/ground_truth/poke/*\")) # 17\n",
    "\n",
    "capsule_fault_imprint_path = natsort.natsorted(glob.glob(base_path + \"capsule/test/fault_imprint/*\"))\n",
    "capsule_fault_imprint_gt_path = natsort.natsorted(glob.glob(base_path + \"capsule/ground_truth/fault_imprint/*\")) # 18\n",
    "\n",
    "capsule_crack_path = natsort.natsorted(glob.glob(base_path + \"capsule/test/crack/*\"))\n",
    "capsule_crack_gt_path = natsort.natsorted(glob.glob(base_path + \"capsule/ground_truth/crack/*\"))# 19\n",
    "\n",
    "# bottle\n",
    "bottle_good_img_path = natsort.natsorted(glob.glob(base_path + \"bottle/train/good/*\"))\n",
    "bottle_good_img_path.extend(natsort.natsorted(glob.glob(base_path + \"bottle/test/good/*\")))\n",
    "\n",
    "bottle_contamination_path = natsort.natsorted(glob.glob(base_path + \"bottle/test/contamination/*\"))\n",
    "bottle_contamination_gt_path = natsort.natsorted(glob.glob(base_path + \"bottle/ground_truth/contamination/*\")) # 20\n",
    "\n",
    "bottle_broken_path = natsort.natsorted(glob.glob(base_path + \"bottle/test/broken_small/*\"))\n",
    "bottle_broken_path.extend(natsort.natsorted(glob.glob(base_path + \"bottle/test/broken_large/*\")))\n",
    "\n",
    "bottle_broken_gt_path = natsort.natsorted(glob.glob(base_path + \"bottle/ground_truth/broken_small/*\"))\n",
    "bottle_broken_gt_path.extend(natsort.natsorted(glob.glob(base_path + \"bottle/ground_truth/broken_large/*\"))) # 21\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def check_paths(name, paths):\n",
    "    \"\"\"\n",
    "    주어진 경로 리스트에서 파일의 존재 여부를 확인하고 디버깅 정보를 출력합니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking paths for: {name}\")\n",
    "    if not paths:\n",
    "        print(f\"[WARNING] No files found for {name}. Please check the path!\")\n",
    "    else:\n",
    "        print(f\"[INFO] {len(paths)} files found for {name}. Example paths:\")\n",
    "        for path in paths[:3]:  # 최대 3개 샘플 경로 출력\n",
    "            print(f\"  {path}\")\n",
    "        # 경로가 실제로 존재하지 않는 파일인지 확인\n",
    "        nonexistent_files = [p for p in paths if not os.path.exists(p)]\n",
    "        if nonexistent_files:\n",
    "            print(f\"[ERROR] {len(nonexistent_files)} files do not exist!\")\n",
    "            for path in nonexistent_files[:3]:  # 최대 3개만 출력\n",
    "                print(f\"  [Missing] {path}\")\n",
    "\n",
    "check_paths(\"toothbrush_good_img_path\", toothbrush_good_img_path)\n",
    "check_paths(\"toothbrush_path\", toothbrush_path)\n",
    "check_paths(\"toothbrush_gt_path\", toothbrush_gt_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 512\n",
    "image_width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import natsort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 메모리 맵 파일 저장 경로\n",
    "memmap_dir = \"memmap_data\"\n",
    "os.makedirs(memmap_dir, exist_ok=True)\n",
    "\n",
    "# 데이터 타입 최적화: uint8 사용\n",
    "image_dtype = np.uint8\n",
    "gt_dtype = np.uint8\n",
    "\n",
    "gt_path_list = [\n",
    "            toothbrush_gt_path,\n",
    "            tile_rough_gt_path,\n",
    "            tile_oil_gt_path,\n",
    "            tile_gray_stroke_gt_path,\n",
    "            tile_glue_strip_gt_path,\n",
    "            tile_crack_gt_path,\n",
    "            screw_thread_gt_path,\n",
    "            screw_scratch_gt_path,\n",
    "            screw_manipulated_gt_path,\n",
    "            pill_scratch_gt_path,\n",
    "            pill_pill_type_gt_path,\n",
    "            pill_faulty_imprint_gt_path,\n",
    "            pill_crack_gt_path,\n",
    "            pill_color_gt_path,\n",
    "            capsule_squeeze_gt_path,\n",
    "            capsule_scratch_gt_path,\n",
    "            capsule_poke_gt_path,\n",
    "            capsule_fault_imprint_gt_path,\n",
    "            capsule_crack_gt_path,\n",
    "            bottle_contamination_gt_path,\n",
    "            bottle_broken_gt_path\n",
    "]\n",
    "gt_list = []\n",
    "for index, gt_path in tqdm(enumerate(gt_path_list)):\n",
    "    gt = []\n",
    "    for i, path in enumerate(gt_path):\n",
    "        tmp = np.array(Image.open(path).resize((image_height, image_width), Image.Resampling.BICUBIC), dtype=gt_dtype)\n",
    "        tmp[np.where(tmp != 0)] = index + 1  # label = index + 1\n",
    "        tmp_zero = np.zeros((image_height, image_width, 21), dtype=gt_dtype)\n",
    "        tmp_zero[:, :, index] = tmp\n",
    "        gt.append(tmp_zero)\n",
    "    gt_list.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = [\n",
    "            toothbrush_path,\n",
    "            tile_rough_path,\n",
    "            tile_oil_path,\n",
    "            tile_gray_stroke_path,\n",
    "            tile_glue_strip_path,\n",
    "            tile_crack_path,\n",
    "            screw_thread_path,\n",
    "            screw_scratch_path,\n",
    "            screw_manipulated_path,\n",
    "            pill_scratch_path,\n",
    "            pill_pill_type_path,\n",
    "            pill_faulty_imprint_path,\n",
    "            pill_crack_path,\n",
    "            pill_color_path,\n",
    "            capsule_squeeze_path,\n",
    "            capsule_scratch_path,\n",
    "            capsule_poke_path,\n",
    "            capsule_fault_imprint_path,\n",
    "            capsule_crack_path,\n",
    "            bottle_contamination_path,\n",
    "            bottle_broken_path\n",
    "]\n",
    "\n",
    "img_list = []\n",
    "for img_path in tqdm(img_path_list):\n",
    "    img = []\n",
    "    for i, path in enumerate(img_path):\n",
    "        tmp = np.array(\n",
    "            Image.open(path).convert(\"RGB\").resize((image_height, image_width), Image.Resampling.BICUBIC),\n",
    "            dtype=image_dtype,\n",
    "        )\n",
    "        if i == 0:\n",
    "            print(tmp.shape)\n",
    "        img.append(tmp)\n",
    "    img_list.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "img_list_sample = img_list\n",
    "gt_list_sample = gt_list\n",
    "\n",
    "# 리스트의 차원별 크기 확인\n",
    "print(f\"Number of classes: {len(img_list_sample)}\")\n",
    "print(f\"Shape of first img: ({len(img_list_sample[0])}, {len(img_list_sample[0][0])}, {len(img_list_sample[0][0][0])})\")\n",
    "print(f\"Shape of first gt: ({len(gt_list_sample[0])}, {len(gt_list_sample[0][0])})\")\n",
    "\n",
    "\n",
    "# 메모리 사용량 확인\n",
    "estimated_memory = sys.getsizeof(img_list_sample) + sys.getsizeof(gt_list_sample)\n",
    "print(f\"Total estimated memory usage: {estimated_memory} bytes\")\n",
    "\n",
    "# 데이터 수집용 리스트\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "\n",
    "# 배치 처리\n",
    "batch_size = 2  # 한 번에 처리할 클래스 수\n",
    "for batch_start in range(0, len(img_list_sample), batch_size):\n",
    "    batch_end = min(batch_start + batch_size, len(img_list_sample))\n",
    "    img_batch = img_list_sample[batch_start:batch_end]\n",
    "    gt_batch = gt_list_sample[batch_start:batch_end]\n",
    "\n",
    "    for class_index, (img, gt) in enumerate(zip(img_batch, gt_batch)):\n",
    "        print(f\"Processing class {batch_start + class_index + 1}/{len(img_list_sample)}...\")\n",
    "        X_train_tmp, X_test_tmp, y_train_tmp, y_test_tmp = train_test_split(\n",
    "            img, gt, test_size=0.2, random_state=42\n",
    "        )\n",
    "        X_train_list.append(X_train_tmp)\n",
    "        X_test_list.append(X_test_tmp)\n",
    "        y_train_list.append(y_train_tmp)\n",
    "        y_test_list.append(y_test_tmp)\n",
    "\n",
    "# 최종 병합\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "# 메모리 맵핑 저장\n",
    "X_train_memmap = np.memmap(\n",
    "    os.path.join(memmap_dir, \"X_train.dat\"), dtype=image_dtype, mode=\"w+\", shape=X_train.shape\n",
    ")\n",
    "y_train_memmap = np.memmap(\n",
    "    os.path.join(memmap_dir, \"y_train.dat\"), dtype=gt_dtype, mode=\"w+\", shape=y_train.shape\n",
    ")\n",
    "\n",
    "X_test_memmap = np.memmap(\n",
    "    os.path.join(memmap_dir, \"X_test.dat\"), dtype=image_dtype, mode=\"w+\", shape=X_test.shape\n",
    ")\n",
    "y_test_memmap = np.memmap(\n",
    "    os.path.join(memmap_dir, \"y_test.dat\"), dtype=gt_dtype, mode=\"w+\", shape=y_test.shape\n",
    ")\n",
    "\n",
    "# 데이터 저장\n",
    "X_train_memmap[:] = X_train[:]\n",
    "y_train_memmap[:] = y_train[:]\n",
    "X_test_memmap[:] = X_test[:]\n",
    "y_test_memmap[:] = y_test[:]\n",
    "\n",
    "# 메모리 맵핑 로드\n",
    "X_train = np.memmap(os.path.join(memmap_dir, \"X_train.dat\"), dtype=image_dtype, mode=\"r\", shape=X_train.shape)\n",
    "y_train = np.memmap(os.path.join(memmap_dir, \"y_train.dat\"), dtype=gt_dtype, mode=\"r\", shape=y_train.shape)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Processing complete!\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형준님 데이터셋 분리에 대한 데이터 로더\n",
    "\n",
    "# CustomDataset 만들어주기\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32).permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
    "        mask = torch.tensor(self.masks[idx], dtype=torch.long)\n",
    "        return image, mask\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    " # DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=21):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # 인코딩\n",
    "        self.enc1_1 = CBR2d(in_channels=in_channels, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = CBR2d(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # 디코딩\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.dec4_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "        self.dec4_2 = CBR2d(in_channels=512, out_channels=256)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.dec3_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        self.dec3_2 = CBR2d(in_channels=256, out_channels=128)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.dec2_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        self.dec2_2 = CBR2d(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.dec1_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        self.dec1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "\n",
    "        # 최종 아웃풋\n",
    "        self.final = nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코딩\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        # 보틀넥\n",
    "        bottleneck = self.bottleneck(pool4)\n",
    "\n",
    "        # 디코딩\n",
    "        unpool4 = self.unpool4(bottleneck)\n",
    "        unpool4 = torch.cat([unpool4, enc4_2], dim=1)\n",
    "        dec4_1 = self.dec4_1(unpool4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        unpool3 = self.unpool3(dec4_2)\n",
    "        unpool3 = torch.cat([unpool3, enc3_2], dim=1)\n",
    "        dec3_1 = self.dec3_1(unpool3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_2)\n",
    "        unpool2 = torch.cat([unpool2, enc2_2], dim=1)\n",
    "        dec2_1 = self.dec2_1(unpool2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_2)\n",
    "        unpool1 = torch.cat([unpool1, enc1_2], dim=1)\n",
    "        dec1_1 = self.dec1_1(unpool1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        out = self.final(dec1_2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 모델과 최적화 함수 정의\n",
    "model = UNet(in_channels=3, out_channels=21).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # 학습률 감소\n",
    "num_epochs = 30\n",
    "\n",
    "# 손실 저장 리스트\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# 누적 스텝 설정 (Gradient Accumulation)\n",
    "accumulation_steps = 4  # 원하는 누적 스텝 수 설정\n",
    "scaler = GradScaler()  # Mixed Precision을 위한 스케일러 생성\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.argmax(dim=3).to(device)  # (batch_size, 512, 512, 21) -> (batch_size, 512, 512)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):  # `device_type`을 명시적으로 설정\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks.long()) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * accumulation_steps  # 축소된 손실 값 보정\n",
    "\n",
    "            # 메모리 관리\n",
    "            del images, masks, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        # 테스트\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in test_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.argmax(dim=3).to(device)  # (batch_size, 512, 512, 21) -> (batch_size, 512, 512)\n",
    "\n",
    "                with autocast(device_type=\"cuda\"):  # 테스트 시에도 혼합 정밀도 사용 가능\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks.long())\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                # 메모리 관리\n",
    "                del images, masks, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "# 모델 학습 시작\n",
    "trained_model = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "# 손실 그래프 그리기\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader에서 첫 번째 배치 확인\n",
    "for images, masks in test_loader:\n",
    "    print(f\"Images shape: {images.shape}\")  # 이미지 텐서 확인\n",
    "    print(f\"Masks shape: {masks.shape}\")    # 정답 마스크 텐서 확인\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation_results(model, test_loader, num_samples=5):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))  # 3개 열로 변경\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)  # 1D -> 2D 배열로 변환\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(test_loader):\n",
    "            if i >= num_samples:  # 필요한 샘플 개수만 처리\n",
    "                break\n",
    "\n",
    "            # GPU로 데이터 이동\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 모델 출력\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Ground Truth와 예측 마스크 변환\n",
    "            masks = masks.argmax(dim=3).cpu().numpy()  # (batch_size, 512, 512, 21) -> (batch_size, 512, 512)\n",
    "            pred_masks = torch.argmax(outputs, dim=1).cpu().numpy()  # (batch_size, 512, 512)\n",
    "\n",
    "            # 입력 이미지 변환\n",
    "            images = images.permute(0, 2, 3, 1).cpu().numpy()  # (batch_size, 3, 512, 512) -> (batch_size, 512, 512, 3)\n",
    "\n",
    "            # 시각화를 위한 이미지 정규화\n",
    "            images = (images - images.min()) / (images.max() - images.min())\n",
    "\n",
    "            # 데이터 시각화\n",
    "            axs[i, 0].imshow(images[0])  # (512, 512, 3)\n",
    "            axs[i, 0].set_title(\"Input Image\")\n",
    "            axs[i, 1].imshow(masks[0], cmap=\"gray\")  # (512, 512)\n",
    "            axs[i, 1].set_title(\"Ground Truth\")\n",
    "            axs[i, 2].imshow(pred_masks[0], cmap=\"gray\", vmin=0, vmax=20)  # (512, 512)\n",
    "            axs[i, 2].set_title(\"Predicted Mask\")\n",
    "\n",
    "            # 배경은 검은색, 예측은 흰색으로 이진화\n",
    "            binary_pred_mask = (pred_masks[0] > 0).astype(float)  # 0은 배경(검은색), 나머지는 흰색\n",
    "            axs[i, 2].imshow(binary_pred_mask, cmap=\"gray\", vmin=0, vmax=1)  # 흰색(1) / 검은색(0)\n",
    "            axs[i, 2].set_title(\"Predicted Mask\")\n",
    "\n",
    "            for ax in axs[i]:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"segmentation_results.png\")  # 파일 저장\n",
    "    print(\"Results saved to 'segmentation_results.png'\")\n",
    "    plt.show()  # 화면 출력\n",
    "    print(\"Segmentation visualization complete.\")\n",
    "\n",
    "# 실행\n",
    "visualize_segmentation_results(model, test_loader, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc_curve(model, test_loader, num_classes=21):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_probs = F.softmax(outputs, dim=1).permute(0, 2, 3, 1).reshape(-1, num_classes)\n",
    "\n",
    "            all_labels.append(masks.cpu().numpy().flatten())\n",
    "            all_preds.append(pred_probs.cpu().numpy().reshape(-1, num_classes))\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Binarize the output for each class\n",
    "    all_labels_bin = label_binarize(all_labels, classes=range(num_classes))\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_preds[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Multi-Class)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"roc_curve.png\")  # 파일 저장\n",
    "    print(\"Results saved to 'roc_curve.png'\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
